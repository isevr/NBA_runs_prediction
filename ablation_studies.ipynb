{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set_theme()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored preprocessing class for ablation\n",
    "\n",
    "class Preprocessing():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        # Feature selection\n",
    "        self.factors = ['Quarter','ShotType','ShotDist','FoulType',\n",
    "                'TimeoutTeam','EnterGame','LeaveGame', 'Shooter',\n",
    "                'Rebounder', 'Blocker','Fouler',\n",
    "                'Fouled','ReboundType','ViolationPlayer', 'ViolationType',\n",
    "                'FreeThrowShooter','TurnoverPlayer','TurnoverType','TurnoverCause',\n",
    "                'TurnoverCauser']\n",
    "\n",
    "    # Label encoding\n",
    "    def encoders(self):\n",
    "        for factor in self.factors:\n",
    "            globals()[f'{factor}_le'] = LabelEncoder()\n",
    "            self.data[factor] = globals()[f'{factor}_le'].fit_transform(self.data[factor])\n",
    "        return self.data\n",
    "\n",
    "    # Creating arrays of runs\n",
    "    def home_runner(self):\n",
    "        run = []\n",
    "        self.home_runs = []\n",
    "        for idx in self.data.index:\n",
    "            if self.data.at[idx,'HomePlay'] is not np.nan:\n",
    "                if 'makes' in self.data.at[idx,'HomePlay']:\n",
    "                    run.append(idx)\n",
    "            elif self.data.at[idx,'AwayPlay'] is not np.nan:\n",
    "                if 'makes' in self.data.at[idx,'AwayPlay']:\n",
    "                    run.clear()\n",
    "            if len(run) == 4:\n",
    "                self.home_runs.append(run.copy())\n",
    "                run.clear()\n",
    "        return self.home_runs\n",
    "                    \n",
    "    def away_runner(self):\n",
    "        run = []\n",
    "        self.away_runs = []\n",
    "        for idx in self.data.index:\n",
    "            if self.data.at[idx,'AwayPlay'] is not np.nan:\n",
    "                if 'makes' in self.data.at[idx,'AwayPlay']:\n",
    "                    run.append(idx)\n",
    "            elif self.data.at[idx,'HomePlay'] is not np.nan:\n",
    "                if 'makes' in self.data.at[idx,'HomePlay']:\n",
    "                    run.clear()\n",
    "            if len(run) == 4:\n",
    "                self.away_runs.append(run.copy())\n",
    "                run.clear()\n",
    "        return self.away_runs\n",
    "\n",
    "    def all_runner(self):\n",
    "        self.all_runs = []\n",
    "        self.all_runs.extend(self.home_runs)\n",
    "        self.all_runs.extend(self.away_runs)\n",
    "        return self.all_runs\n",
    "\n",
    "    # Flattening runs\n",
    "    def runs_iter(self, removed_feature=None):\n",
    "        if removed_feature:\n",
    "            features = [f for f in self.factors if f != removed_feature]\n",
    "        else:\n",
    "            features = self.factors\n",
    "        \n",
    "        fact_cols = [col + str((i // len(features)) % 10 + 1) for i, col in enumerate(features * 10)]\n",
    "        fact_cols.append('class')\n",
    "        \n",
    "        self.runs_df = pd.DataFrame()\n",
    "        for run in self.all_runs:\n",
    "            a = self.data.loc[run[0]-10:run[0]-1, features].values.ravel()\n",
    "            a = np.append(a,1)\n",
    "            self.runs_df = pd.concat([self.runs_df,pd.DataFrame([a.copy()])])\n",
    "        self.runs_df.columns = fact_cols\n",
    "        return self.runs_df\n",
    "\n",
    "    # Function to remove runs from original Dataframe\n",
    "    def no_runs_preprocessing(self):\n",
    "        r = [i[0] for i in self.all_runs]  \n",
    "        r_x = []\n",
    "        for num in r:\n",
    "            r_x.extend(range(num - 10, num + 1))\n",
    "        self.no_runs_df = self.data[~self.data.index.isin(r_x)].reset_index(drop=True)\n",
    "        segment_size = 10\n",
    "        segments = len(self.no_runs_df) // segment_size\n",
    "        self.no_runs_split = np.array_split(self.no_runs_df, segments)\n",
    "        self.no_runs_split = [x for x in self.no_runs_split if len(x) == 10]\n",
    "        return self.no_runs_split\n",
    "\n",
    "    # Flattening no runs\n",
    "    def no_runs_optimized(self, removed_feature=None):\n",
    "        if removed_feature:\n",
    "            features = [f for f in self.factors if f != removed_feature]\n",
    "        else:\n",
    "            features = self.factors\n",
    "        \n",
    "        fact_cols = [col + str((i // len(features)) % 10 + 1) for i, col in enumerate(features * 10)]\n",
    "        fact_cols.append('class')\n",
    "        \n",
    "        self.no_runs_df = pd.DataFrame([np.append(segment.loc[:, features].values.ravel(), int(0)) for segment in self.no_runs_split])\n",
    "        self.no_runs_df.columns = fact_cols\n",
    "        self.no_runs_df = self.no_runs_df.sample(frac=0.14)\n",
    "        return self.no_runs_df\n",
    "\n",
    "    # Preparing final Dataframe for training\n",
    "    def final(self, removed_feature=None):\n",
    "        self.runs_iter(removed_feature)\n",
    "        self.no_runs_optimized(removed_feature)\n",
    "        self.final_df = pd.concat([self.runs_df,self.no_runs_df],ignore_index=True).dropna().astype(int)\n",
    "        self.scaler = MinMaxScaler((0,255))\n",
    "        self.values = pd.DataFrame(self.scaler.fit_transform(self.final_df.iloc[:,:-1]))\n",
    "        self.labels = self.final_df.iloc[:,-1]\n",
    "        self.values.to_csv('data.csv')\n",
    "        self.labels.to_csv('labels.csv')\n",
    "\n",
    "    # Run everything\n",
    "    def preprocess(self, removed_feature=None):\n",
    "        self.encoders()\n",
    "        self.home_runner()\n",
    "        self.away_runner()\n",
    "        self.all_runner()\n",
    "        self.no_runs_preprocessing()\n",
    "        self.final(removed_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concat datasets and preprocess\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):  \n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        data = pd.concat([data, pd.read_csv(file_path)], ignore_index=True)\n",
    "\n",
    "preprocessor = Preprocessing(data)\n",
    "\n",
    "# Initialize ablation study results\n",
    "train_results = {}\n",
    "val_results = {}\n",
    "\n",
    "# Run ablation study\n",
    "for feature in preprocessor.factors:\n",
    "    print(f\"Running ablation study by removing feature: {feature}\")\n",
    "    \n",
    "    # Preprocess data without the selected feature\n",
    "    preprocessor.preprocess(removed_feature=feature)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    X = pd.read_csv('data.csv', index_col=0)\n",
    "    X = X.values.reshape(-1, 19,10)  # 19 features * 10 rows = 190\n",
    "    X = X / 255\n",
    "    y = pd.read_csv('labels.csv', index_col=0)\n",
    "    y = keras.utils.to_categorical(y, 2)\n",
    "\n",
    "    # Model training\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(19,10,1)))\n",
    "    model.add(keras.layers.Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))   \n",
    "    model.add(keras.layers.MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Conv2D(25,(3,3), strides=1, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units=512, activation=\"relu\"))\n",
    "    # model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "    # model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "    # model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['accuracy','precision','recall','f1_score'], optimizer='rmsprop')\n",
    "\n",
    "    history = model.fit(X, y, epochs=20, verbose=1, validation_split=0.2, callbacks=[early_stop])\n",
    "    \n",
    "    # Save results\n",
    "    train_results[feature] = {\n",
    "        'loss': np.round(np.mean(history.history['loss'])),\n",
    "        'accuracy': np.round(np.mean(history.history['accuracy'])),\n",
    "        'precision': np.round(np.mean(history.history['precision'])),\n",
    "        'recall': np.round(np.mean(history.history['recall'])),\n",
    "        'f1_score': np.round(np.mean(history.history['f1_score']))\n",
    "    }\n",
    "\n",
    "    val_results[feature] = {\n",
    "        'loss': np.round(np.mean(history.history['val_loss'])),\n",
    "        'accuracy': np.round(np.mean(history.history['val_accuracy'])),\n",
    "        'precision': np.round(np.mean(history.history['val_precision'])),\n",
    "        'recall': np.round(np.mean(history.history['val_recall'])),\n",
    "        'f1_score': np.round(np.mean(history.history['val_f1_score']))\n",
    "    }\n",
    "\n",
    "    # Plot results for each ablation\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Validation Loss (Removing {feature})')\n",
    "    plt.legend(['train','validation'],loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Validation Accuracy (Removing {feature})')\n",
    "    plt.legend(['train','validation'],loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save pickle\n",
    "\n",
    "import pickle\n",
    "with open('ablation_train_results.pkl', 'wb') as f:\n",
    "    pickle.dump(train_results, f)\n",
    "\n",
    "with open('ablation_val_results.pkl', 'wb') as f:\n",
    "    pickle.dump(val_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ablation_train_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
